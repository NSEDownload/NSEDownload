<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>NSEDownload.scraper API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>NSEDownload.scraper</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import json
import math
import pandas as pd
import re
import requests
import threading
from bs4 import BeautifulSoup
from io import StringIO
from requests.adapters import HTTPAdapter, Retry

from NSEDownload.static_data import get_headers, get_adjusted_headers, get_symbol_mapping_url, get_company_events_url, \
    get_symbol_count_url

interim_dfs = []


def process_window(stage, url):
    response = make_get_request(url)
    try:
        interim_dfs[stage] = process_html_response(response)
    except AttributeError:
        pass


def process_html_response(response):
    page_content = BeautifulSoup(response, &#34;html.parser&#34;)
    lines = page_content.find(id=&#34;csvContentDiv&#34;).get_text()
    lines = lines.replace(&#39;:&#39;, &#34;, \n&#34;)
    df = pd.read_csv(StringIO(lines))
    df.set_index(&#34;Date&#34;, inplace=True)
    df = df[::-1]
    return df


def make_get_request(url):
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
    session.mount(&#39;https://&#39;, HTTPAdapter(max_retries=retries))
    response = session.get(url, timeout=10, headers=get_headers())
    session.close()
    return response.content


def scrape_data(start_date, end_date, request_type,
                index_name=None, url=None, stock_symbol=None, symbol_count=None, series=&#34;EQ&#34;):
    &#34;&#34;&#34;Called by stocks and indices to scrape data. Create threads for different requests, parses data, combines them and returns dataframe

    Args:
        start_date (datetime): start date
        end_date (datetime): end date
        request_type (str): Either &#39;stock&#39; or &#39;index&#39;
        index_name (str, optional): If type index then this gives name of index. Defaults to None.
        url (str, optional): URL to scrape from. Defaults to None.
        stock_symbol (str, optional): If type stock then this gives stock symbol. Defaults to None.
        symbol_count (str, optional): Intermediate variable needed for scraping. Defaults to None.
        series(str, optional): By default set to EQ, but can choose any series or All

    Returns:
        Pandas DataFrame: df containing data for stocksymbol for provided date range
    &#34;&#34;&#34;

    total_stages = math.ceil((end_date - start_date).days / 365)
    global interim_dfs
    interim_dfs = [pd.DataFrame()] * total_stages

    threads = []
    for stage in range(total_stages):

        window_start_date = start_date + stage * datetime.timedelta(days=365)
        window_end_date = window_start_date + datetime.timedelta(days=364)

        if window_start_date &gt; end_date:
            break

        if window_end_date &gt; end_date:
            window_end_date = end_date

        if request_type == &#39;stock&#39;:
            final_url = get_symbol_mapping_url() + &#39;?symbol=&#39; + stock_symbol + &#39;&amp;segmentLink=3&amp;symbolCount&#39; \
                        + symbol_count + &#34;&amp;series=&#34; + series + &#34;&amp;dateRange=+&amp;fromDate=&#34; + \
                        window_start_date.strftime(
                            &#34;%d-%m-%Y&#34;) + &#34;&amp;toDate=&#34; + window_end_date.strftime(
                &#34;%d-%m-%Y&#34;) + &#34;&amp;dataType=PRICEVOLUMEDELIVERABLE&#34;

        if request_type == &#39;index&#39;:
            final_url = url + &#39;?indexType=&#39; + index_name + \
                        &#39;&amp;fromDate=&#39; + \
                        window_start_date.strftime(&#34;%d-%m-%Y&#34;) + &#39;&amp;toDate=&#39; + \
                        window_end_date.strftime(&#34;%d-%m-%Y&#34;)

        thread = threading.Thread(target=process_window, args=[stage, final_url])
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    result = pd.DataFrame()
    for stage in range(total_stages):
        result = pd.concat([result, interim_dfs[stage]])
    result.index = pd.to_datetime(result.index)
    result.sort_index(inplace=True)

    return result


def add_quotes_to_field(match):
    match = match.group()
    return match[0] + &#39;&#34;&#39; + match[1:-1] + &#39;&#34;:&#39;


def scrape_bonus_splits(symbol):
    &#34;&#34;&#34;Scrapes for bonuses and splits

    Args:
        symbol (str): Stock Symbol

    Returns:
        list: Returns list of dates of event and ratio of original and new price
    &#34;&#34;&#34;

    event_dates, event_ratio = [], []
    url_more_than_24 = get_company_events_url() + symbol + \
                       &#34;&amp;Industry=&amp;ExDt=More%20than%2024%20&#34; \
                       &#34;Months&amp;exDt=More%20than%2024%20Months&#34; \
                       &#34;&amp;recordDt=&amp;bcstartDt=&amp;industry&#34; \
                       &#34;=&amp;CAType=&#34;

    url_last_24 = get_company_events_url() + symbol + \
                  &#34;&amp;Industry=&amp;ExDt=Last%2012%20Months&#34; \
                  &#34;&amp;exDt=Last%2012%20Months&amp;&#34; \
                  &#34;&amp;recordDt=&amp;bcstartDt=&amp;industry&#34; \
                  &#34;=&amp;CAType=&#34;

    for url in [url_more_than_24, url_last_24]:

        response = requests.get(url, timeout=60, headers=get_adjusted_headers())
        page_content = &#34;{&#34; + BeautifulSoup(response.content, &#34;html.parser&#34;).text.replace(&#39;\n&#39;, &#39;&#39;).replace(&#39;\t&#39;, &#39;&#39;)[16:]
        json_input = re.sub(r&#39;[{,][a-zA-Z]+:&#39;, add_quotes_to_field, page_content)
        json_content = json.loads(json_input)
        corporate_actions = json_content[&#34;rows&#34;]

        for row in corporate_actions:

            subject = row[&#34;sub&#34;].lower()
            date = row[&#34;exDt&#34;]
            if date not in event_dates:

                # Scraping for Splits
                if subject.find(&#34;split&#34;) != -1 or subject.find(&#34;division&#34;) != -1:
                    num = re.findall(&#39;\d+&#39;, subject)
                    if len(num) &lt; 2:
                        continue
                    event_ratio.append(int(num[0]) / int(num[1]))
                    event_dates.append(date)
                    print(&#34;Split event on: &#34; + date)

                # Scraping for Bonus
                if subject.find(&#34;bonus&#34;) != -1:
                    num = re.findall(&#39;\d+&#39;, subject)
                    if len(num) &lt; 2:
                        continue
                    event_ratio.append((int(num[0]) + int(num[1])) / int(num[1]))
                    event_dates.append(date)
                    print(&#34;Bonus event on: &#34; + date)

    return [event_ratio, event_dates]


def scrape_symbol(symbol):
    &#34;&#34;&#34;Scraping intermediate variable symbol Count

    Args:
        symbol (str): Stock Symbol

    Raises:
        SystemExit: Exit on any exception to request

    Returns:
        str: Symbol Count
    &#34;&#34;&#34;

    try:
        response = requests.post(get_symbol_count_url(),
                                 data={&#34;symbol&#34;: symbol},
                                 headers=get_headers(),
                                 timeout=60)

    except requests.exceptions.RequestException as e:
        raise SystemExit(e)

    if response.status_code != requests.codes.ok:
        response.raise_for_status()

    return str(BeautifulSoup(response.content, &#34;html.parser&#34;))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="NSEDownload.scraper.add_quotes_to_field"><code class="name flex">
<span>def <span class="ident">add_quotes_to_field</span></span>(<span>match)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_quotes_to_field(match):
    match = match.group()
    return match[0] + &#39;&#34;&#39; + match[1:-1] + &#39;&#34;:&#39;</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.make_get_request"><code class="name flex">
<span>def <span class="ident">make_get_request</span></span>(<span>url)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_get_request(url):
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
    session.mount(&#39;https://&#39;, HTTPAdapter(max_retries=retries))
    response = session.get(url, timeout=10, headers=get_headers())
    session.close()
    return response.content</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.process_html_response"><code class="name flex">
<span>def <span class="ident">process_html_response</span></span>(<span>response)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_html_response(response):
    page_content = BeautifulSoup(response, &#34;html.parser&#34;)
    lines = page_content.find(id=&#34;csvContentDiv&#34;).get_text()
    lines = lines.replace(&#39;:&#39;, &#34;, \n&#34;)
    df = pd.read_csv(StringIO(lines))
    df.set_index(&#34;Date&#34;, inplace=True)
    df = df[::-1]
    return df</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.process_window"><code class="name flex">
<span>def <span class="ident">process_window</span></span>(<span>stage, url)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_window(stage, url):
    response = make_get_request(url)
    try:
        interim_dfs[stage] = process_html_response(response)
    except AttributeError:
        pass</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.scrape_bonus_splits"><code class="name flex">
<span>def <span class="ident">scrape_bonus_splits</span></span>(<span>symbol)</span>
</code></dt>
<dd>
<div class="desc"><p>Scrapes for bonuses and splits</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>symbol</code></strong> :&ensp;<code>str</code></dt>
<dd>Stock Symbol</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Returns list of dates of event and ratio of original and new price</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_bonus_splits(symbol):
    &#34;&#34;&#34;Scrapes for bonuses and splits

    Args:
        symbol (str): Stock Symbol

    Returns:
        list: Returns list of dates of event and ratio of original and new price
    &#34;&#34;&#34;

    event_dates, event_ratio = [], []
    url_more_than_24 = get_company_events_url() + symbol + \
                       &#34;&amp;Industry=&amp;ExDt=More%20than%2024%20&#34; \
                       &#34;Months&amp;exDt=More%20than%2024%20Months&#34; \
                       &#34;&amp;recordDt=&amp;bcstartDt=&amp;industry&#34; \
                       &#34;=&amp;CAType=&#34;

    url_last_24 = get_company_events_url() + symbol + \
                  &#34;&amp;Industry=&amp;ExDt=Last%2012%20Months&#34; \
                  &#34;&amp;exDt=Last%2012%20Months&amp;&#34; \
                  &#34;&amp;recordDt=&amp;bcstartDt=&amp;industry&#34; \
                  &#34;=&amp;CAType=&#34;

    for url in [url_more_than_24, url_last_24]:

        response = requests.get(url, timeout=60, headers=get_adjusted_headers())
        page_content = &#34;{&#34; + BeautifulSoup(response.content, &#34;html.parser&#34;).text.replace(&#39;\n&#39;, &#39;&#39;).replace(&#39;\t&#39;, &#39;&#39;)[16:]
        json_input = re.sub(r&#39;[{,][a-zA-Z]+:&#39;, add_quotes_to_field, page_content)
        json_content = json.loads(json_input)
        corporate_actions = json_content[&#34;rows&#34;]

        for row in corporate_actions:

            subject = row[&#34;sub&#34;].lower()
            date = row[&#34;exDt&#34;]
            if date not in event_dates:

                # Scraping for Splits
                if subject.find(&#34;split&#34;) != -1 or subject.find(&#34;division&#34;) != -1:
                    num = re.findall(&#39;\d+&#39;, subject)
                    if len(num) &lt; 2:
                        continue
                    event_ratio.append(int(num[0]) / int(num[1]))
                    event_dates.append(date)
                    print(&#34;Split event on: &#34; + date)

                # Scraping for Bonus
                if subject.find(&#34;bonus&#34;) != -1:
                    num = re.findall(&#39;\d+&#39;, subject)
                    if len(num) &lt; 2:
                        continue
                    event_ratio.append((int(num[0]) + int(num[1])) / int(num[1]))
                    event_dates.append(date)
                    print(&#34;Bonus event on: &#34; + date)

    return [event_ratio, event_dates]</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.scrape_data"><code class="name flex">
<span>def <span class="ident">scrape_data</span></span>(<span>start_date, end_date, request_type, index_name=None, url=None, stock_symbol=None, symbol_count=None, series='EQ')</span>
</code></dt>
<dd>
<div class="desc"><p>Called by stocks and indices to scrape data. Create threads for different requests, parses data, combines them and returns dataframe</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>start_date</code></strong> :&ensp;<code>datetime</code></dt>
<dd>start date</dd>
<dt><strong><code>end_date</code></strong> :&ensp;<code>datetime</code></dt>
<dd>end date</dd>
<dt><strong><code>request_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Either 'stock' or 'index'</dd>
<dt><strong><code>index_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If type index then this gives name of index. Defaults to None.</dd>
<dt><strong><code>url</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>URL to scrape from. Defaults to None.</dd>
<dt><strong><code>stock_symbol</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If type stock then this gives stock symbol. Defaults to None.</dd>
<dt><strong><code>symbol_count</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Intermediate variable needed for scraping. Defaults to None.</dd>
</dl>
<p>series(str, optional): By default set to EQ, but can choose any series or All</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Pandas DataFrame</code></dt>
<dd>df containing data for stocksymbol for provided date range</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_data(start_date, end_date, request_type,
                index_name=None, url=None, stock_symbol=None, symbol_count=None, series=&#34;EQ&#34;):
    &#34;&#34;&#34;Called by stocks and indices to scrape data. Create threads for different requests, parses data, combines them and returns dataframe

    Args:
        start_date (datetime): start date
        end_date (datetime): end date
        request_type (str): Either &#39;stock&#39; or &#39;index&#39;
        index_name (str, optional): If type index then this gives name of index. Defaults to None.
        url (str, optional): URL to scrape from. Defaults to None.
        stock_symbol (str, optional): If type stock then this gives stock symbol. Defaults to None.
        symbol_count (str, optional): Intermediate variable needed for scraping. Defaults to None.
        series(str, optional): By default set to EQ, but can choose any series or All

    Returns:
        Pandas DataFrame: df containing data for stocksymbol for provided date range
    &#34;&#34;&#34;

    total_stages = math.ceil((end_date - start_date).days / 365)
    global interim_dfs
    interim_dfs = [pd.DataFrame()] * total_stages

    threads = []
    for stage in range(total_stages):

        window_start_date = start_date + stage * datetime.timedelta(days=365)
        window_end_date = window_start_date + datetime.timedelta(days=364)

        if window_start_date &gt; end_date:
            break

        if window_end_date &gt; end_date:
            window_end_date = end_date

        if request_type == &#39;stock&#39;:
            final_url = get_symbol_mapping_url() + &#39;?symbol=&#39; + stock_symbol + &#39;&amp;segmentLink=3&amp;symbolCount&#39; \
                        + symbol_count + &#34;&amp;series=&#34; + series + &#34;&amp;dateRange=+&amp;fromDate=&#34; + \
                        window_start_date.strftime(
                            &#34;%d-%m-%Y&#34;) + &#34;&amp;toDate=&#34; + window_end_date.strftime(
                &#34;%d-%m-%Y&#34;) + &#34;&amp;dataType=PRICEVOLUMEDELIVERABLE&#34;

        if request_type == &#39;index&#39;:
            final_url = url + &#39;?indexType=&#39; + index_name + \
                        &#39;&amp;fromDate=&#39; + \
                        window_start_date.strftime(&#34;%d-%m-%Y&#34;) + &#39;&amp;toDate=&#39; + \
                        window_end_date.strftime(&#34;%d-%m-%Y&#34;)

        thread = threading.Thread(target=process_window, args=[stage, final_url])
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    result = pd.DataFrame()
    for stage in range(total_stages):
        result = pd.concat([result, interim_dfs[stage]])
    result.index = pd.to_datetime(result.index)
    result.sort_index(inplace=True)

    return result</code></pre>
</details>
</dd>
<dt id="NSEDownload.scraper.scrape_symbol"><code class="name flex">
<span>def <span class="ident">scrape_symbol</span></span>(<span>symbol)</span>
</code></dt>
<dd>
<div class="desc"><p>Scraping intermediate variable symbol Count</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>symbol</code></strong> :&ensp;<code>str</code></dt>
<dd>Stock Symbol</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>SystemExit</code></dt>
<dd>Exit on any exception to request</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Symbol Count</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_symbol(symbol):
    &#34;&#34;&#34;Scraping intermediate variable symbol Count

    Args:
        symbol (str): Stock Symbol

    Raises:
        SystemExit: Exit on any exception to request

    Returns:
        str: Symbol Count
    &#34;&#34;&#34;

    try:
        response = requests.post(get_symbol_count_url(),
                                 data={&#34;symbol&#34;: symbol},
                                 headers=get_headers(),
                                 timeout=60)

    except requests.exceptions.RequestException as e:
        raise SystemExit(e)

    if response.status_code != requests.codes.ok:
        response.raise_for_status()

    return str(BeautifulSoup(response.content, &#34;html.parser&#34;))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="NSEDownload" href="index.html">NSEDownload</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="NSEDownload.scraper.add_quotes_to_field" href="#NSEDownload.scraper.add_quotes_to_field">add_quotes_to_field</a></code></li>
<li><code><a title="NSEDownload.scraper.make_get_request" href="#NSEDownload.scraper.make_get_request">make_get_request</a></code></li>
<li><code><a title="NSEDownload.scraper.process_html_response" href="#NSEDownload.scraper.process_html_response">process_html_response</a></code></li>
<li><code><a title="NSEDownload.scraper.process_window" href="#NSEDownload.scraper.process_window">process_window</a></code></li>
<li><code><a title="NSEDownload.scraper.scrape_bonus_splits" href="#NSEDownload.scraper.scrape_bonus_splits">scrape_bonus_splits</a></code></li>
<li><code><a title="NSEDownload.scraper.scrape_data" href="#NSEDownload.scraper.scrape_data">scrape_data</a></code></li>
<li><code><a title="NSEDownload.scraper.scrape_symbol" href="#NSEDownload.scraper.scrape_symbol">scrape_symbol</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>